data:
  batch_size: 8
  dataset: openwebtext
model:  
  block_size: 1024
  vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
  n_layer: 12
  n_head: 12
  n_embd: 768
  dropout: 0.0
  bias: True
optimization:
  device: cuda:1
  lr: 3e-4
  run_name: 2023-02-02-day-3-gpt-small-copy-openwebtext-adam-dropout-0.0